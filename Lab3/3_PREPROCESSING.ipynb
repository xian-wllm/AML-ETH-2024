{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d229119a3e7d8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.9/14.9 MB\u001B[0m \u001B[31m82.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting scipy>=1.9\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.2/41.2 MB\u001B[0m \u001B[31m43.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging>=21 in /usr/lib/python3/dist-packages (from scikit-image) (21.3)\n",
      "Collecting numpy>=1.23\n",
      "  Downloading numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.4/16.4 MB\u001B[0m \u001B[31m81.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting imageio>=2.33\n",
      "  Downloading imageio-2.36.1-py3-none-any.whl (315 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m315.4/315.4 KB\u001B[0m \u001B[31m47.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting networkx>=2.8\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m115.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tifffile>=2022.8.12\n",
      "  Downloading tifffile-2024.9.20-py3-none-any.whl (228 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m228.2/228.2 KB\u001B[0m \u001B[31m53.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pillow>=9.1\n",
      "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.4/4.4 MB\u001B[0m \u001B[31m112.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting lazy-loader>=0.4\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: pillow, numpy, networkx, lazy-loader, tifffile, scipy, imageio, scikit-image\n",
      "Successfully installed imageio-2.36.1 lazy-loader-0.4 networkx-3.4.2 numpy-2.2.0 pillow-11.0.0 scikit-image-0.24.0 scipy-1.14.1 tifffile-2024.9.20\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "id": "5acb027c0de37d9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:19:45.361043Z",
     "start_time": "2024-12-12T15:19:45.357937Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the base path\n",
    "base_path = './tibo1/Lab3/'\n",
    "# Set the working directory to the project root\n",
    "#os.chdir('./tibo1/Lab3/')\n",
    "\n",
    "# Convert to absolute path and add to sys.path\n",
    "sys.path.append(os.path.abspath(base_path))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "75c9d0be5acbb1b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:19:47.476237Z",
     "start_time": "2024-12-12T15:19:45.936790Z"
    }
   },
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import filters\n",
    "import torch\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "from utils import uint2single, single2tensor4, test_onesplit, tensor2uint, get_device"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "f9bb53153a4f9463",
   "metadata": {},
   "source": [
    "### 1. Pre-Definitions\n",
    "#### 1a. Loader Function for the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "423c1c9a94cb7cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:19:48.907316Z",
     "start_time": "2024-12-12T15:19:48.903849Z"
    }
   },
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "    return loaded_object"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "5434fa123a64eaac",
   "metadata": {},
   "source": [
    "#### 1b. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "4dcec82c551f8bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:19:49.884770Z",
     "start_time": "2024-12-12T15:19:49.877435Z"
    }
   },
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def pad_image(image):\n",
    "    ## Ensure image is a proper numpy array\n",
    "    #if not isinstance(image, np.ndarray):\n",
    "    #    raise ValueError(\"Input must be a numpy array.\")\n",
    "#\n",
    "    ## Ensure image is contiguous in memory\n",
    "    #if not image.flags['C_CONTIGUOUS']:\n",
    "    #    image = np.ascontiguousarray(image)\n",
    "#\n",
    "    ## Ensure image has correct dtype\n",
    "    #image = image.astype(np.uint8)\n",
    "#\n",
    "    ## Check dimensions\n",
    "    #if len(image.shape) != 2:\n",
    "    #    raise ValueError(f\"Image must be grayscale (2D). Got shape: {image.shape}\")\n",
    "\n",
    "    # Compute padding\n",
    "    h, w = image.shape\n",
    "    target_size = max(h, w)\n",
    "    padded_image = cv2.copyMakeBorder(\n",
    "        image,\n",
    "        0, target_size - h,\n",
    "        0, target_size - w,\n",
    "        cv2.BORDER_CONSTANT,\n",
    "        value=0\n",
    "    )\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "def heal_image(img: np.ndarray):\n",
    "    # Function to remove small artifacts from the image\n",
    "    assert len(img.shape) == 2, \"Image must be grayscale\"\n",
    "    height, width = img.shape\n",
    "    ret, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    min_patch_size = 30\n",
    "    for contour in contours:\n",
    "        contour_area = cv2.contourArea(contour)\n",
    "        if contour_area < min_patch_size:\n",
    "            mask = np.zeros((height, width), np.uint8)\n",
    "            cv2.drawContours(mask, [contour], 0, 255, -1)\n",
    "            img = cv2.inpaint(img, mask, 2, cv2.INPAINT_TELEA)\n",
    "    return img\n",
    "\n",
    "def denoise_enhance_image( image: np.ndarray, model, noise_level_model_tensor, device, denoise_before: bool) -> np.ndarray:\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(4, 4))\n",
    "    if denoise_before:\n",
    "        image = denoise_image(image, model, noise_level_model_tensor, device)\n",
    "        image = clahe.apply(image)\n",
    "    else:\n",
    "        image = clahe.apply(image)\n",
    "        image = denoise_image(image, model, noise_level_model_tensor, device)\n",
    "    image = heal_image(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_image( image: np.ndarray, target_size: Tuple[int, int], model, noise_level_model_tensor, device, is_test: bool = False, denoise_before: bool = False) -> np.ndarray:\n",
    "    assert len(image.shape) == 2, 'Image must be grayscale'\n",
    "    # Pad image to create square aspect ratio\n",
    "    padded_image = pad_image(image)\n",
    "    # TODO SHOULD WE ALSO DENOISE TEST OR NOT\n",
    "    if not is_test:\n",
    "        padded_image = denoise_enhance_image(\n",
    "            padded_image, model, noise_level_model_tensor, device, denoise_before\n",
    "        )\n",
    "    # Resize image to target size\n",
    "    resized = cv2.resize(padded_image, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "def process_mask(mask: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:\n",
    "    assert len(mask.shape) == 2, 'Mask must be grayscale'\n",
    "    # Pad image to create square aspect ratio\n",
    "    padded_mask = pad_image(mask)\n",
    "    # Apply bilateral filter\n",
    "    padded_mask = cv2.bilateralFilter(padded_mask, d=9, sigmaColor=1, sigmaSpace=1)\n",
    "    # Threshold the filtered image\n",
    "    threshold_value = filters.threshold_otsu(padded_mask)\n",
    "    padded_mask = (padded_mask > threshold_value * 2).astype(np.uint8) * 255  # Threshold\n",
    "    # Dilate the filtered image\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    padded_mask = cv2.dilate(padded_mask, kernel, iterations=1)\n",
    "    # Apply Gaussian blur to the dilated and filtered image\n",
    "    padded_mask = cv2.GaussianBlur(padded_mask, (3, 3), 2)\n",
    "    padded_mask = (padded_mask > threshold_value).astype(np.uint8) * 255\n",
    "    # Resize image to target size\n",
    "    resized = cv2.resize(padded_mask, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "    return resized\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "c8434d39c1e3e8a2",
   "metadata": {},
   "source": [
    "#### 1c. Load DRUNet model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7f1015748f83233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:19:51.278398Z",
     "start_time": "2024-12-12T15:19:51.262543Z"
    }
   },
   "source": [
    "# Load the pre-trained DRUNet model\n",
    "from drunet.drunet import UNetRes\n",
    "\n",
    "def load_drunet_model(model_path: str, device: str = 'cpu'):\n",
    "    print(\"Running on device: \", device)\n",
    "    n_channels = 1\n",
    "    noise_level_model = 5\n",
    "    noise_level_model_tensor = torch.FloatTensor([noise_level_model / 255.]).to(device)\n",
    "\n",
    "    model = UNetRes(\n",
    "        in_nc=n_channels + 1,\n",
    "        out_nc=n_channels,\n",
    "        nc=[64, 128, 256, 512],\n",
    "        nb=4,\n",
    "        act_mode='R',\n",
    "        downsample_mode=\"strideconv\",\n",
    "        upsample_mode=\"convtranspose\"\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True), strict=True)\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model = model.to(device)\n",
    "    return model, noise_level_model_tensor\n",
    "\n",
    "def denoise_image(image: np.ndarray, model, noise_level_model_tensor, device) -> np.ndarray:\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "    image = uint2single(image)\n",
    "    image = single2tensor4(image)\n",
    "\n",
    "    # Ensure the noise_level_model_tensor is on the same device as the input\n",
    "    noise_level_model_tensor = noise_level_model_tensor.to(device)\n",
    "    image = image.to(device)  # Move image to the correct device\n",
    "\n",
    "    image = torch.cat((\n",
    "        image,\n",
    "        noise_level_model_tensor.repeat(1, 1, image.shape[2], image.shape[3])\n",
    "    ), dim=1)\n",
    "    image = image.to(device)\n",
    "    denoised_img = test_onesplit(model, image, refield=32)\n",
    "    denoised_img = tensor2uint(denoised_img)\n",
    "    return denoised_img"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "cb26d9f7581ba339",
   "metadata": {},
   "source": [
    "### 2. Preprocess Data\n",
    "#### 2.a Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "fdd2de83c92ba66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:19:58.348626Z",
     "start_time": "2024-12-12T15:19:52.479855Z"
    }
   },
   "source": [
    "# Paths to your data files\n",
    "train_data_path = './data/train.pkl'\n",
    "test_data_path = './data/test.pkl'\n",
    "\n",
    "# Load the data\n",
    "train_data = load_zipped_pickle(train_data_path)\n",
    "test_data = load_zipped_pickle(test_data_path)\n",
    "\n",
    "print(f'Loaded {len(train_data)} patients from training data.')\n",
    "print(f'Loaded {len(test_data)} patients from test data.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 65 patients from training data.\n",
      "Loaded 20 patients from test data.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "81b4ea1fedea5ba6",
   "metadata": {},
   "source": [
    "#### 2.b Initialize the DRUNet model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a87c67be6c94823a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:19:58.810568Z",
     "start_time": "2024-12-12T15:19:58.430353Z"
    }
   },
   "source": [
    "# Path to the pretrained DRUNet model\n",
    "drunet_model_path = './drunet/drunet_gray.pth'\n",
    "device = get_device()\n",
    "print(f'Running on device: {device}')\n",
    "\n",
    "# Load the model\n",
    "drunet_model, noise_level_model_tensor = load_drunet_model(drunet_model_path, device)\n",
    "TARGET_SIZE = (224, 224)  # Adjust as needed currently 2 times amateur samples dim"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n",
      "Running on device:  mps\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "a8b17a6b6074af73",
   "metadata": {},
   "source": [
    "#### 2.c Preprocess Training Data\n",
    "##### Separate Amateur and Expert Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "1a406ad61b7b29c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:20:02.293174Z",
     "start_time": "2024-12-12T15:20:02.290043Z"
    }
   },
   "source": [
    "# Separate amateur and expert data\n",
    "amateur_data = [patient for patient in train_data if patient['dataset'] == 'amateur']\n",
    "expert_data = [patient for patient in train_data if patient['dataset'] == 'expert']\n",
    "\n",
    "print(f'Expert data: {len(expert_data)} patients')\n",
    "print(f'Amateur data: {len(amateur_data)} patients')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert data: 19 patients\n",
      "Amateur data: 46 patients\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "15dd553f36443613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:20:02.818203Z",
     "start_time": "2024-12-12T15:20:02.814938Z"
    }
   },
   "source": [
    "def stratified_bootstrap(y_train, frame_indexes):\n",
    "    num_bootstrap_samples = y_train.shape[0] - len(frame_indexes)\n",
    "    print(f'Num of bootstrap samples {num_bootstrap_samples}')\n",
    "    print(f'Frame indices:{frame_indexes}')\n",
    "\n",
    "    # perform bootstrapping\n",
    "    bootstrap_mask_indexes = np.random.choice(frame_indexes, size=num_bootstrap_samples, replace=True)\n",
    "    bootstrap_masks = y_train[bootstrap_mask_indexes]\n",
    "\n",
    "    return bootstrap_masks, bootstrap_mask_indexes"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "84de393452de159",
   "metadata": {},
   "source": [
    "##### Process Images and Masks"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T15:31:43.972548Z",
     "start_time": "2024-12-12T15:31:43.961534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_dataset(data: List[dict], target_size: Tuple[int, int], drunet_model, noise_level_model_tensor, device, is_test: bool = False):\n",
    "    print(\"Running on device: \", device)\n",
    "    total_frames = 0\n",
    "\n",
    "    for i, patient in enumerate(tqdm(data, desc=f'Processing {\"Test\" if is_test else \"Train\"} Data: ')):\n",
    "        video = patient['video']\n",
    "\n",
    "        if not is_test:\n",
    "            masks = patient['label']\n",
    "            frames = patient['frames']\n",
    "\n",
    "            # Process images\n",
    "            new_video = [\n",
    "                process_image(\n",
    "                    video[..., frame].astype(np.uint8),\n",
    "                    target_size,\n",
    "                    drunet_model,\n",
    "                    noise_level_model_tensor,\n",
    "                    device,\n",
    "                    is_test=False,\n",
    "                    denoise_before=False\n",
    "                )\n",
    "                for frame in range(video.shape[-1])\n",
    "            ]\n",
    "\n",
    "            # Augmentation: denoise before enhancement\n",
    "            new_video += [\n",
    "                process_image(\n",
    "                    video[..., frame].astype(np.uint8),\n",
    "                    target_size,\n",
    "                    drunet_model,\n",
    "                    noise_level_model_tensor,\n",
    "                    device,\n",
    "                    is_test=False,\n",
    "                    denoise_before=True\n",
    "                )\n",
    "                for frame in range(video.shape[-1])\n",
    "            ]\n",
    "\n",
    "            new_video = np.array(new_video) / 255.0  # Normalize images\n",
    "\n",
    "\n",
    "            new_masks = [\n",
    "                process_mask(masks[..., frame].astype(np.uint8), target_size)\n",
    "                                  for frame in range(masks.shape[-1])] * 2  # Duplicate masks for augmented images\n",
    "            new_masks = np.array(new_masks).astype(np.float32)\n",
    "\n",
    "            # Needed for the bootstrap to sample also from the Augmentation subset\n",
    "            patient['frame'] = np.concatenate((frames, np.array([frame + video.shape[-1] for frame in frames])), axis=0)\n",
    "            print(\"New frame shape after augmentation: \", patient['frame'])\n",
    "            print(\"Patient shape after augmentation: \", patient['frame'].shape)\n",
    "\n",
    "            bootstrapped_masks, bootstrap_indices = stratified_bootstrap(new_masks, frames)\n",
    "            print(\"Bootstrapped masks shape: \", bootstrapped_masks.shape)\n",
    "            print(\"Bootstrapped indices shape: \", bootstrap_indices.shape)\n",
    "            print(\"Number of frames of video before bootstrapping: \", new_video.shape[0])\n",
    "            print(\"Number of frames of masks before bootstrapping: \", new_masks.shape[0])\n",
    "            bootstrapped_images = new_video[bootstrap_indices]\n",
    "\n",
    "            print(\"Bootstrapped images shape: \", bootstrapped_images.shape)\n",
    "\n",
    "            # Concatenate bootstrapped data\n",
    "            new_masks = np.concatenate((new_masks, bootstrapped_masks), axis=0)\n",
    "            new_video = np.concatenate((new_video, bootstrapped_images), axis=0)\n",
    "\n",
    "            # double the video and masks\n",
    "            new_masks = np.concatenate((new_masks, new_masks), axis=0).astype(np.float32)\n",
    "            new_video = np.concatenate((new_video, new_video), axis=0).astype(np.float32)\n",
    "\n",
    "            patient['video'] = new_video\n",
    "            patient['label'] = new_masks\n",
    "\n",
    "        else:\n",
    "            # Test data processing (no masks)\n",
    "            new_video = [\n",
    "                process_image(\n",
    "                    video[..., frame].astype(np.uint8),\n",
    "                    target_size,\n",
    "                    drunet_model,\n",
    "                    noise_level_model_tensor,\n",
    "                    device,\n",
    "                    is_test=True,\n",
    "                    denoise_before=False\n",
    "                )\n",
    "                for frame in range(video.shape[-1])\n",
    "            ]\n",
    "            new_video = np.array(new_video) / 255.0  # Normalize images\n",
    "\n",
    "            patient['video'] = new_video\n",
    "\n",
    "        total_frames += new_video.shape[0]\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Processed {i+1}/{len(data)} patients. Total frames so far: {total_frames}\")\n",
    "\n",
    "    print(\"Finished preprocessing and bootstrapping.\")\n",
    "    return data, total_frames"
   ],
   "id": "25987875c69726b3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "56d265021a5ce626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T16:02:27.829224Z",
     "start_time": "2024-12-12T15:31:45.397333Z"
    }
   },
   "source": [
    "preprocessed_expert_data, expert_total_frames = preprocess_dataset(expert_data, TARGET_SIZE, drunet_model, noise_level_model_tensor, device, is_test=False)\n",
    "#preprocessed_amateur_data, amateur_total_frames = preprocess_dataset(amateur_data, TARGET_SIZE, drunet_model, noise_level_model_tensor, device, is_test=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:   0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [ 20  34  68 244 258 292]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 445\n",
      "Frame indices:[20, 34, 68]\n",
      "Bootstrapped masks shape:  (445, 224, 224)\n",
      "Bootstrapped indices shape:  (445,)\n",
      "Number of frames of video before bootstrapping:  448\n",
      "Number of frames of masks before bootstrapping:  448\n",
      "Bootstrapped images shape:  (445, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:   5%|▌         | 1/19 [03:24<1:01:15, 204.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/19 patients. Total frames so far: 1786\n",
      "New frame shape after augmentation:  [  6  20  60  89 103 143]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 163\n",
      "Frame indices:[6, 20, 60]\n",
      "Bootstrapped masks shape:  (163, 224, 224)\n",
      "Bootstrapped indices shape:  (163,)\n",
      "Number of frames of video before bootstrapping:  166\n",
      "Number of frames of masks before bootstrapping:  166\n",
      "Bootstrapped images shape:  (163, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  16%|█▌        | 3/19 [06:54<33:25, 125.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [ 29  42  53  83  96 107]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 105\n",
      "Frame indices:[29, 42, 53]\n",
      "Bootstrapped masks shape:  (105, 224, 224)\n",
      "Bootstrapped indices shape:  (105,)\n",
      "Number of frames of video before bootstrapping:  108\n",
      "Number of frames of masks before bootstrapping:  108\n",
      "Bootstrapped images shape:  (105, 224, 224)\n",
      "New frame shape after augmentation:  [  6  26  62  96 116 152]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 177\n",
      "Frame indices:[6, 26, 62]\n",
      "Bootstrapped masks shape:  (177, 224, 224)\n",
      "Bootstrapped indices shape:  (177,)\n",
      "Number of frames of video before bootstrapping:  180\n",
      "Number of frames of masks before bootstrapping:  180\n",
      "Bootstrapped images shape:  (177, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  21%|██        | 4/19 [09:28<34:08, 136.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [ 24  30  66 118 124 160]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 185\n",
      "Frame indices:[24, 30, 66]\n",
      "Bootstrapped masks shape:  (185, 224, 224)\n",
      "Bootstrapped indices shape:  (185,)\n",
      "Number of frames of video before bootstrapping:  188\n",
      "Number of frames of masks before bootstrapping:  188\n",
      "Bootstrapped images shape:  (185, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  26%|██▋       | 5/19 [12:03<33:25, 143.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [  4  31  54 120 147 170]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 229\n",
      "Frame indices:[4, 31, 54]\n",
      "Bootstrapped masks shape:  (229, 224, 224)\n",
      "Bootstrapped indices shape:  (229,)\n",
      "Number of frames of video before bootstrapping:  232\n",
      "Number of frames of masks before bootstrapping:  232\n",
      "Bootstrapped images shape:  (229, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  32%|███▏      | 6/19 [15:51<37:17, 172.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6/19 patients. Total frames so far: 5252\n",
      "New frame shape after augmentation:  [  1  10  48  72  81 119]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 139\n",
      "Frame indices:[1, 10, 48]\n",
      "Bootstrapped masks shape:  (139, 224, 224)\n",
      "Bootstrapped indices shape:  (139,)\n",
      "Number of frames of video before bootstrapping:  142\n",
      "Number of frames of masks before bootstrapping:  142\n",
      "Bootstrapped images shape:  (139, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  37%|███▋      | 7/19 [17:54<31:12, 156.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [ 26  59  73 111 144 158]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 167\n",
      "Frame indices:[26, 59, 73]\n",
      "Bootstrapped masks shape:  (167, 224, 224)\n",
      "Bootstrapped indices shape:  (167,)\n",
      "Number of frames of video before bootstrapping:  170\n",
      "Number of frames of masks before bootstrapping:  170\n",
      "Bootstrapped images shape:  (167, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  42%|████▏     | 8/19 [19:35<25:22, 138.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [ 19  23  60  89  93 130]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 137\n",
      "Frame indices:[19, 23, 60]\n",
      "Bootstrapped masks shape:  (137, 224, 224)\n",
      "Bootstrapped indices shape:  (137,)\n",
      "Number of frames of video before bootstrapping:  140\n",
      "Number of frames of masks before bootstrapping:  140\n",
      "Bootstrapped images shape:  (137, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  47%|████▋     | 9/19 [21:16<21:07, 126.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [ 24  57  72 103 136 151]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 155\n",
      "Frame indices:[24, 57, 72]\n",
      "Bootstrapped masks shape:  (155, 224, 224)\n",
      "Bootstrapped indices shape:  (155,)\n",
      "Number of frames of video before bootstrapping:  158\n",
      "Number of frames of masks before bootstrapping:  158\n",
      "Bootstrapped images shape:  (155, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  53%|█████▎    | 10/19 [22:38<16:57, 113.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [  0  23  35  75  98 110]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 147\n",
      "Frame indices:[0, 23, 35]\n",
      "Bootstrapped masks shape:  (147, 224, 224)\n",
      "Bootstrapped indices shape:  (147,)\n",
      "Number of frames of video before bootstrapping:  150\n",
      "Number of frames of masks before bootstrapping:  150\n",
      "Bootstrapped images shape:  (147, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  58%|█████▊    | 11/19 [24:42<15:32, 116.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11/19 patients. Total frames so far: 8262\n",
      "New frame shape after augmentation:  [  2   7  13  97 102 108]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 187\n",
      "Frame indices:[2, 7, 13]\n",
      "Bootstrapped masks shape:  (187, 224, 224)\n",
      "Bootstrapped indices shape:  (187,)\n",
      "Number of frames of video before bootstrapping:  190\n",
      "Number of frames of masks before bootstrapping:  190\n",
      "Bootstrapped images shape:  (187, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  63%|██████▎   | 12/19 [27:39<15:44, 134.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frame shape after augmentation:  [  3  30  77 104 131 178]\n",
      "Patient shape after augmentation:  (6,)\n",
      "Num of bootstrap samples 199\n",
      "Frame indices:[3, 30, 77]\n",
      "Bootstrapped masks shape:  (199, 224, 224)\n",
      "Bootstrapped indices shape:  (199,)\n",
      "Number of frames of video before bootstrapping:  202\n",
      "Number of frames of masks before bootstrapping:  202\n",
      "Bootstrapped images shape:  (199, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data:  68%|██████▊   | 13/19 [30:42<14:10, 141.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m preprocessed_expert_data, expert_total_frames \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpert_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTARGET_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrunet_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise_level_model_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_test\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#preprocessed_amateur_data, amateur_total_frames = preprocess_dataset(amateur_data, TARGET_SIZE, drunet_model, noise_level_model_tensor, device, is_test=False)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[16], line 14\u001B[0m, in \u001B[0;36mpreprocess_dataset\u001B[0;34m(data, target_size, drunet_model, noise_level_model_tensor, device, is_test)\u001B[0m\n\u001B[1;32m     10\u001B[0m frames \u001B[38;5;241m=\u001B[39m patient[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Process images\u001B[39;00m\n\u001B[1;32m     13\u001B[0m new_video \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m---> 14\u001B[0m     \u001B[43mprocess_image\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvideo\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muint8\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdrunet_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnoise_level_model_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_test\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdenoise_before\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m frame \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(video\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     24\u001B[0m ]\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Augmentation: denoise before enhancement\u001B[39;00m\n\u001B[1;32m     27\u001B[0m new_video \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     28\u001B[0m     process_image(\n\u001B[1;32m     29\u001B[0m         video[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, frame]\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m frame \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(video\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     38\u001B[0m ]\n",
      "Cell \u001B[0;32mIn[4], line 63\u001B[0m, in \u001B[0;36mprocess_image\u001B[0;34m(image, target_size, model, noise_level_model_tensor, device, is_test, denoise_before)\u001B[0m\n\u001B[1;32m     61\u001B[0m padded_image \u001B[38;5;241m=\u001B[39m pad_image(image)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# if not is_test:\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m padded_image \u001B[38;5;241m=\u001B[39m \u001B[43mdenoise_enhance_image\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadded_image\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise_level_model_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenoise_before\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# Resize image to target size\u001B[39;00m\n\u001B[1;32m     67\u001B[0m resized \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(padded_image, target_size, interpolation\u001B[38;5;241m=\u001B[39mcv2\u001B[38;5;241m.\u001B[39mINTER_LINEAR)\n",
      "Cell \u001B[0;32mIn[4], line 54\u001B[0m, in \u001B[0;36mdenoise_enhance_image\u001B[0;34m(image, model, noise_level_model_tensor, device, denoise_before)\u001B[0m\n\u001B[1;32m     52\u001B[0m     image \u001B[38;5;241m=\u001B[39m clahe\u001B[38;5;241m.\u001B[39mapply(image)\n\u001B[1;32m     53\u001B[0m     image \u001B[38;5;241m=\u001B[39m denoise_image(image, model, noise_level_model_tensor, device)\n\u001B[0;32m---> 54\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mheal_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image\n",
      "Cell \u001B[0;32mIn[4], line 43\u001B[0m, in \u001B[0;36mheal_image\u001B[0;34m(img)\u001B[0m\n\u001B[1;32m     41\u001B[0m         mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((height, width), np\u001B[38;5;241m.\u001B[39muint8)\n\u001B[1;32m     42\u001B[0m         cv2\u001B[38;5;241m.\u001B[39mdrawContours(mask, [contour], \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 43\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minpaint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mINPAINT_TELEA\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "191e9d0c2a68a48c",
   "metadata": {},
   "source": [
    "#### 2.e Train-Validation Split for Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8985a3723cd24606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train/val split. Total expert frames: 13478, \n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting train/val split. Total expert frames: {expert_total_frames}, \")\n",
    "            #f\"Total amateur frames: {amateur_total_frames}\")\n",
    "\n",
    "def train_val_split(data: list, nbr_total_frames: int, split_ratio=0.8) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    train_images, train_masks, val_images, val_masks = [], [], [], []\n",
    "    train_frames = 0\n",
    "    train_limit = nbr_total_frames * split_ratio  # Precompute the training frame limit\n",
    "\n",
    "    print(f\"Starting train/val split. Total frames: {nbr_total_frames}. Train split ratio: {split_ratio:.2f}\")\n",
    "    for patient in tqdm(data, desc=\"Splitting patients into train/val\"):\n",
    "        if train_frames < train_limit:\n",
    "            train_images.append(patient['video'])\n",
    "            train_masks.append(patient['label'])\n",
    "            train_frames += patient['video'].shape[0]\n",
    "        else:\n",
    "            val_images.append(patient['video'])\n",
    "            val_masks.append(patient['label'])\n",
    "\n",
    "    # Combine lists into arrays\n",
    "    train_images = np.concatenate(train_images, axis=0)\n",
    "    train_masks = np.concatenate(train_masks, axis=0)\n",
    "    val_images = np.concatenate(val_images, axis=0)\n",
    "    val_masks = np.concatenate(val_masks, axis=0)\n",
    "\n",
    "    print(f\"Finished train/val split: Train frames: {train_images.shape[0]}, Val frames: {val_images.shape[0]}\")\n",
    "    return train_images, train_masks, val_images, val_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59470d473534a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train/val split. Total frames: 13478. Train split ratio: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting patients into train/val: 100%|██████████| 19/19 [00:00<00:00, 228999.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished train/val split: Train frames: 10798, Val frames: 2680\n"
     ]
    }
   ],
   "source": [
    "expert_train_images, expert_train_masks, expert_val_images, expert_val_masks = train_val_split(preprocessed_expert_data, expert_total_frames)\n",
    "#amateur_train_images, amateur_train_masks, amateur_val_images, amateur_val_masks = train_val_split(preprocessed_amateur_data, amateur_total_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e03c81ebea890",
   "metadata": {},
   "source": [
    "#### 2.f Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cd9f4958ea3753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save expert data\n",
    "np.save('./tibo1/Lab3/out/preprocessed/expert_X_train.npy', expert_train_images)\n",
    "np.save('./tibo1/Lab3/out/preprocessed/expert_y_train.npy', expert_train_masks)\n",
    "np.save('./tibo1/Lab3/out/preprocessed/expert_X_val.npy', expert_val_images)\n",
    "np.save('./tibo1/Lab3/out/preprocessed/expert_y_val.npy', expert_val_masks)\n",
    "\n",
    "# Save amateur data\n",
    "# np.save('./tibo1/Lab3/out/preprocessed/amateur_X_train.npy', amateur_train_images)\n",
    "# np.save('./tibo1/Lab3/out/preprocessed/amateur_y_train.npy', amateur_train_masks)\n",
    "# np.save('./tibo1/Lab3/out/preprocessed/amateur_X_val.npy', amateur_val_images)\n",
    "# np.save('./tibo1/Lab3/out/preprocessed/amateur_y_val.npy', amateur_val_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d834117650123",
   "metadata": {},
   "source": [
    "#### 2g. Preprocess test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a60cf85cea16ad",
   "metadata": {},
   "source": [
    "| Step                  | Train/Validation Dataset           | Test Dataset                        |\n",
    "|-----------------------|-------------------------------------|--------------------------------------|\n",
    "| Padding               | Yes                                | Yes                                 |\n",
    "| Resizing              | Yes                                | Yes                                 |\n",
    "| Augmentation          | Yes (e.g., rotation, affine, etc.) | No                                  |\n",
    "| Bootstrapping         | Yes                                | No                                  |\n",
    "| Denoising/Enhancement | Yes                                | Yes                                 |\n",
    "| Data Duplication      | Yes                                | No                                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a67e70a002dfb54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:47:20.933782Z",
     "start_time": "2024-12-08T23:45:06.498821Z"
    }
   },
   "source": [
    "test_images, _ = preprocess_dataset(test_data, TARGET_SIZE, drunet_model, noise_level_model_tensor, device, is_test=True)\n",
    "\n",
    "# Save test data\n",
    "np.save('./tibo1/Lab3/out/preprocessed/test.npy', test_images)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data:   5%|▌         | 1/20 [00:06<02:03,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/20 patients. Total frames so far: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data:  30%|███       | 6/20 [00:38<01:30,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6/20 patients. Total frames so far: 1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data:  55%|█████▌    | 11/20 [01:09<00:56,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11/20 patients. Total frames so far: 2464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data:  80%|████████  | 16/20 [01:42<00:25,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16/20 patients. Total frames so far: 3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data: 100%|██████████| 20/20 [02:08<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing and bootstrapping.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ac322d467ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drunet_model.detach()\n",
    "if get_device() == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "elif get_device() == 'mps':\n",
    "    os.system('mps free')\n",
    "else:\n",
    "    print(\"No action taken.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
