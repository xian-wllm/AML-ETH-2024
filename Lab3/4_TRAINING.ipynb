{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T18:24:50.014746Z",
     "start_time": "2024-12-16T18:24:47.971467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!hostname\n",
    "!pip install pandas==2.2.3 \\\n",
    "xgboost==2.1.2 \\\n",
    "catboost==1.2.7 \\\n",
    "lightgbm==4.5.0 \\\n",
    "loky==3.4.1 \\\n",
    "scikit-learn==1.5.2 \\\n",
    "joblib==1.4.2 \\\n",
    "seaborn==0.13.2 \\\n",
    "kaggle==1.6.17 \\\n",
    "tqdm==4.66.6 \\\n",
    "colorama==0.4.6 \\\n",
    "biosppy==0.8.0 \\\n",
    "neurokit2==0.2.10 \\\n",
    "imbalanced-learn==0.12.4 \\\n",
    "pywavelets==1.7.0 \\\n",
    "entropy==0.1.5 \\\n",
    "torch==2.5.1 \\\n",
    "torchvision==0.20.1\n"
   ],
   "id": "3471b2fff76666b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193-122-153-173\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: pandas==2.2.3 in ./.local/lib/python3.10/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: xgboost==2.1.2 in ./.local/lib/python3.10/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: catboost==1.2.7 in ./.local/lib/python3.10/site-packages (1.2.7)\r\n",
      "Requirement already satisfied: lightgbm==4.5.0 in ./.local/lib/python3.10/site-packages (4.5.0)\r\n",
      "Requirement already satisfied: loky==3.4.1 in ./.local/lib/python3.10/site-packages (3.4.1)\r\n",
      "Requirement already satisfied: scikit-learn==1.5.2 in ./.local/lib/python3.10/site-packages (1.5.2)\r\n",
      "Requirement already satisfied: joblib==1.4.2 in ./.local/lib/python3.10/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: seaborn==0.13.2 in ./.local/lib/python3.10/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: kaggle==1.6.17 in ./.local/lib/python3.10/site-packages (1.6.17)\r\n",
      "Requirement already satisfied: tqdm==4.66.6 in ./.local/lib/python3.10/site-packages (4.66.6)\r\n",
      "Requirement already satisfied: colorama==0.4.6 in ./.local/lib/python3.10/site-packages (0.4.6)\r\n",
      "Requirement already satisfied: biosppy==0.8.0 in ./.local/lib/python3.10/site-packages (0.8.0)\r\n",
      "Requirement already satisfied: neurokit2==0.2.10 in ./.local/lib/python3.10/site-packages (0.2.10)\r\n",
      "Requirement already satisfied: imbalanced-learn==0.12.4 in ./.local/lib/python3.10/site-packages (0.12.4)\r\n",
      "Requirement already satisfied: pywavelets==1.7.0 in ./.local/lib/python3.10/site-packages (1.7.0)\r\n",
      "Requirement already satisfied: entropy==0.1.5 in ./.local/lib/python3.10/site-packages (0.1.5)\r\n",
      "Requirement already satisfied: torch==2.5.1 in ./.local/lib/python3.10/site-packages (2.5.1)\r\n",
      "Requirement already satisfied: torchvision==0.20.1 in ./.local/lib/python3.10/site-packages (0.20.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas==2.2.3) (2024.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas==2.2.3) (2.9.0.post0)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.local/lib/python3.10/site-packages (from pandas==2.2.3) (1.26.4)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas==2.2.3) (2022.1)\r\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.10/site-packages (from xgboost==2.1.2) (1.14.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.local/lib/python3.10/site-packages (from xgboost==2.1.2) (2.21.5)\r\n",
      "Requirement already satisfied: graphviz in ./.local/lib/python3.10/site-packages (from catboost==1.2.7) (0.20.3)\r\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from catboost==1.2.7) (3.5.1)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from catboost==1.2.7) (1.16.0)\r\n",
      "Requirement already satisfied: plotly in ./.local/lib/python3.10/site-packages (from catboost==1.2.7) (5.24.1)\r\n",
      "Requirement already satisfied: cloudpickle in ./.local/lib/python3.10/site-packages (from loky==3.4.1) (3.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3/dist-packages (from scikit-learn==1.5.2) (3.1.0)\r\n",
      "Requirement already satisfied: certifi>=2023.7.22 in ./.local/lib/python3.10/site-packages (from kaggle==1.6.17) (2024.12.14)\r\n",
      "Requirement already satisfied: bleach in /usr/lib/python3/dist-packages (from kaggle==1.6.17) (4.1.0)\r\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle==1.6.17) (2.25.1)\r\n",
      "Requirement already satisfied: python-slugify in ./.local/lib/python3.10/site-packages (from kaggle==1.6.17) (8.0.4)\r\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle==1.6.17) (1.26.5)\r\n",
      "Requirement already satisfied: bidict in ./.local/lib/python3.10/site-packages (from biosppy==0.8.0) (0.23.1)\r\n",
      "Requirement already satisfied: opencv-python in ./.local/lib/python3.10/site-packages (from biosppy==0.8.0) (4.10.0.84)\r\n",
      "Requirement already satisfied: h5py in /usr/lib/python3/dist-packages (from biosppy==0.8.0) (3.6.0)\r\n",
      "Requirement already satisfied: shortuuid in ./.local/lib/python3.10/site-packages (from biosppy==0.8.0) (1.0.13)\r\n",
      "Requirement already satisfied: setuptools>=42.0.2 in /usr/lib/python3/dist-packages (from entropy==0.1.5) (59.6.0)\r\n",
      "Requirement already satisfied: docopt>=0.6.2 in ./.local/lib/python3.10/site-packages (from entropy==0.1.5) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\r\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.5.1) (3.0.3)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (9.1.0.70)\r\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch==2.5.1) (3.6.0)\r\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (3.1.0)\r\n",
      "Requirement already satisfied: fsspec in /usr/lib/python3/dist-packages (from torch==2.5.1) (2024.3.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/lib/python3/dist-packages (from torch==2.5.1) (4.9.0)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\r\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (3.4.2)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.10/site-packages (from torch==2.5.1) (10.3.5.147)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.10/site-packages (from torchvision==0.20.1) (11.0.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\r\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from plotly->catboost==1.2.7) (21.3)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.local/lib/python3.10/site-packages (from plotly->catboost==1.2.7) (9.0.0)\r\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./.local/lib/python3.10/site-packages (from python-slugify->kaggle==1.6.17) (1.3)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:29:31.495271Z",
     "start_time": "2024-12-08T23:29:31.488060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "base_path = './tibo1/Lab3/'\n",
    "# Set the working directory to the project root\n",
    "#os.chdir('./tibo1/Lab3/')\n",
    "\n",
    "# Convert to absolute path and add to sys.path\n",
    "sys.path.append(os.path.abspath(base_path))"
   ],
   "id": "9a2b8c560c10b251",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:29:35.213726Z",
     "start_time": "2024-12-08T23:29:32.138519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from utils import get_device\n",
    "from torch import Tensor\n",
    "from dataset import MyDataset\n",
    "import multiprocessing\n",
    "from model import UNet\n",
    "import torch"
   ],
   "id": "959d0ff2f31a17bb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Training\n",
    "#### 1a. Load the Expert Data"
   ],
   "id": "69f66ad5cba83b51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:29:39.304463Z",
     "start_time": "2024-12-08T23:29:35.217765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load expert data\n",
    "expert_X_train = np.load('./tibo1/Lab3/out/preprocessed/expert_X_train.npy')\n",
    "expert_y_train = np.load('./tibo1/Lab3/out/preprocessed/expert_y_train.npy')\n",
    "expert_X_val = np.load('./tibo1/Lab3/out/preprocessed/expert_X_val.npy')\n",
    "expert_y_val = np.load('./tibo1/Lab3/out/preprocessed/expert_y_val.npy')"
   ],
   "id": "ea632a99acbda61d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:39:16.895376Z",
     "start_time": "2024-12-08T23:39:16.891716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Example Usage\n",
    "device = get_device().type\n",
    "use_amp = device == 'cuda'  # Use AMP only if on CUDA\n",
    "BATCH_SIZE = 16\n"
   ],
   "id": "40cde705d0c53be0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1b. Create the Dataloaders with Transformations and Data Augmentation",
   "id": "2862429c1f326036"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:41:33.237009Z",
     "start_time": "2024-12-08T23:41:33.232355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the number of CPU cores available\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MyDataset(expert_X_train, expert_y_train, transform=True)\n",
    "val_dataset = MyDataset(expert_X_val, expert_y_val)  # No transform for validation\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(f\"Using {num_workers} workers for DataLoader\")\n"
   ],
   "id": "7c77e7d984ee1f3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 30 workers for DataLoader\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1c. Define the Model (U-Net)\n",
    "\n",
    "| **Aspect**             | **Concatenation**                         | **Addition**                              |\n",
    "|-------------------------|-------------------------------------------|-------------------------------------------|\n",
    "| **Feature Combination** | Combines all features (doubles channels). | Merges features (no change in channels).  |\n",
    "| **Parameter Efficiency**| More parameters due to increased channels.| Fewer parameters; efficient.              |\n",
    "| **Memory Usage**        | Higher memory usage.                      | Lower memory usage.                       |\n",
    "| **Use Case**            | Tasks requiring high precision (e.g., segmentation).| Tasks prioritizing efficiency or residual learning. |\n",
    "\n",
    "\n",
    "**Dimension Example**\n",
    "\n",
    "For an input of $1 \\times 1 \\times 256 \\times 256$ (batch size, channels, height, width):\n",
    "\n",
    "| Step | Output Dimensions (Your UNet and Modular UNet) |\n",
    "| --- | --- |\n",
    "| Conv1 | 64×256×25664 \\times 256 \\times 25664×256×256 |\n",
    "| Pool1 | 64×128×12864 \\times 128 \\times 12864×128×128 |\n",
    "| Conv2 | 128×128×128128 \\times 128 \\times 128128×128×128 |\n",
    "| Pool2 | 128×64×64128 \\times 64 \\times 64128×64×64 |\n",
    "| Conv3 | 256×64×64256 \\times 64 \\times 64256×64×64 |\n",
    "| Pool3 | 256×32×32256 \\times 32 \\times 32256×32×32 |\n",
    "| Conv4 | 512×32×32512 \\times 32 \\times 32512×32×32 |\n",
    "| Pool4 | 512×16×16512 \\times 16 \\times 16512×16×16 |\n",
    "| Conv5 | 1024×16×161024 \\times 16 \\times 161024×16×16 |\n",
    "| Up6 + Skip4 | 1024×32×321024 \\times 32 \\times 321024×32×32 |\n",
    "| Conv6 | 512×32×32512 \\times 32 \\times 32512×32×32 |\n",
    "| Up7 + Skip3 | 512×64×64512 \\times 64 \\times 64512×64×64 |\n",
    "| Conv7 | 256×64×64256 \\times 64 \\times 64256×64×64 |\n",
    "| Up8 + Skip2 | 256×128×128256 \\times 128 \\times 128256×128×128 |\n",
    "| Conv8 | 128×128×128128 \\times 128 \\times 128128×128×128 |\n",
    "| Up9 + Skip1 | 128×256×256128 \\times 256 \\times 256128×256×256 |\n",
    "| Conv9 | 64×256×25664 \\times 256 \\times 25664×256×256 |\n",
    "| Output Conv | n_classes×256×256n\\_classes \\times 256 \\times 256n_classes×256×256 |\n"
   ],
   "id": "906f90efa8b9a6c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:29:41.543594Z",
     "start_time": "2024-12-08T23:29:41.008674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = UNet(n_classes=1, n_channels=1, bilinear=False)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to(device)"
   ],
   "id": "71f00414b7956402",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1d. Define the Loss Function and Optimizer",
   "id": "cf40489cb8601bbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:29:43.085444Z",
     "start_time": "2024-12-08T23:29:43.080055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def power_jaccard_loss(y_true, y_pred, p=2, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    From https://www.scitepress.org/Papers/2021/103040/103040.pdf\n",
    "    \"\"\"\n",
    "    y_true = y_true.float().view(-1)\n",
    "    y_pred = y_pred.float().view(-1)\n",
    "\n",
    "    intersection = (y_true * y_pred).sum()\n",
    "    total = (torch.pow(y_true, p) + torch.pow(y_pred, p)).sum()\n",
    "    union = total - intersection\n",
    "\n",
    "    IoU = (intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return 1 - IoU\n",
    "\n",
    "# Set up optimizer\n",
    "LEARNING_RATE = 3e-4\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-8, momentum=0.999)\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)\n",
    "\n",
    "# Set up loss function\n",
    "criterion = power_jaccard_loss"
   ],
   "id": "ec2ecbacc3ec7aa5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1g. Metrics for Evaluation\n",
    "\n",
    "**Dice Coefficient**\n",
    "- **Intuition** : Measures the overlap between predicted and ground truth masks. Values range from 0 (no overlap) to 1 (perfect overlap). Useful for imbalanced datasets.\n",
    "\n",
    "- **Formula** :\n",
    "$$\n",
    " \\text{Dice Coefficient} = \\frac{2 \\cdot |A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Dice Loss**\n",
    "- **Intuition** : Penalizes poor segmentation overlap. Loss decreases as overlap improves.\n",
    "\n",
    "- **Formula** :\n",
    "$$\n",
    " \\text{Dice Loss} = 1 - \\text{Dice Coefficient}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**IoU Coefficient**\n",
    "- **Intuition** : Ratio of overlap to total area. Complements Dice for evaluating segmentation.\n",
    "\n",
    "- **Formula** :\n",
    "$$\n",
    " \\text{IoU} = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}\n",
    "$$"
   ],
   "id": "f33d7a080b8edbe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T23:29:44.493858Z",
     "start_time": "2024-12-08T23:29:44.488826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the Dice coefficient and Dice loss\n",
    "def dice_coeff(input: Tensor, target: Tensor, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    iou = iou_coeff(input, target, smooth=epsilon)\n",
    "    return (2 * iou) / (iou + 1)\n",
    "\n",
    "def dice_loss(input: Tensor, target: Tensor):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    return 1 - dice_coeff(input, target)\n",
    "\n",
    "def iou_coeff(y_true: Tensor, y_pred: Tensor, smooth=1e-6):\n",
    "    y_true = y_true.float().view(-1)\n",
    "    y_pred = y_pred.float().view(-1)\n",
    "\n",
    "    # intersection is equivalent to True Positive count\n",
    "    # union is the mutually inclusive area of all labels & predictions\n",
    "    intersection = (y_true * y_pred).sum()\n",
    "    total = (y_true + y_pred).sum()\n",
    "    union = total - intersection\n",
    "\n",
    "    return (intersection + smooth) / (union + smooth)"
   ],
   "id": "16bd92c1b501236f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1e. Training Loop",
   "id": "5d5a64b585d3bf17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T00:00:03.397582Z",
     "start_time": "2024-12-08T23:41:41.323617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import traceback\n",
    "\n",
    "\n",
    "# Directories for checkpoints and outputs\n",
    "checkpoint_dir = './tibo1/Lab3/out/checkpoints/'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Initialize tracking variables\n",
    "last_checkpoint = None\n",
    "last_epoch = 0\n",
    "\n",
    "# Search for the latest checkpoint\n",
    "for file_name in os.listdir(checkpoint_dir):\n",
    "    match = re.match(r'checkpoint_epoch(\\d+)\\.pth', file_name)\n",
    "    if match:\n",
    "        epoch_num = int(match.group(1))\n",
    "        if epoch_num > last_epoch:\n",
    "            last_epoch = epoch_num\n",
    "            last_checkpoint = os.path.join(checkpoint_dir, file_name)\n",
    "\n",
    "# Resume training if a checkpoint is found\n",
    "if last_checkpoint:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    model.load_state_dict(torch.load(last_checkpoint), weights_only=True)\n",
    "    optimizer.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'optimizer_epoch{last_epoch}.pth'), weights_only=True))\n",
    "    print(f\"Resumed from epoch {last_epoch}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "# Update starting epoch\n",
    "start_epoch = last_epoch + 1\n",
    "\n",
    "# Number of epochs for training\n",
    "EPOCHS = 40\n",
    "best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "\n",
    "train_losses = globals().get('train_losses', [])  # Resume losses if exists\n",
    "val_losses = globals().get('val_losses', [])\n",
    "val_dices = globals().get('val_dices', [])\n",
    "val_ious = globals().get('val_ious', [])\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "print(\"Running on device:\", device)\n",
    "print(\"Batch Size: \", BATCH_SIZE)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch}/{EPOCHS}', unit='batch') as pbar:\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            pbar.update()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    dice_score = 0\n",
    "    iou_score = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute metrics\n",
    "            outputs = (outputs > 0.5).float()\n",
    "            dice_score += dice_coeff(outputs, masks).item()\n",
    "            iou_score += iou_coeff(outputs, masks).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    dice_score /= len(val_loader)\n",
    "    iou_score /= len(val_loader)\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(dice_score)\n",
    "\n",
    "    # Save metrics\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    val_losses.append(val_loss)\n",
    "    val_dices.append(dice_score)\n",
    "    val_ious.append(iou_score)\n",
    "\n",
    "    print(f'Epoch {epoch}: Train Loss {train_losses[-1]:.4f}, Val Loss {val_losses[-1]:.4f}, Dice {dice_score:.4f}, IoU {iou_score:.4f}')\n",
    "\n",
    "    # Save model and optimizer checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}.pth'))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, f'optimizer_epoch{epoch}.pth'))\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = os.path.join(checkpoint_dir, f'best_model_epoch_{epoch}.pth')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved with validation loss {best_val_loss:.4f} at epoch {epoch}\")"
   ],
   "id": "4ec67cd96ebb5f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting training from scratch.\n",
      "Running on device: cuda\n",
      "Batch Size:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: 100%|██████████| 675/675 [01:15<00:00,  8.97batch/s, loss=0.838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 0.7495, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n",
      "Best model saved with validation loss 0.7798 at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|██████████| 675/675 [01:13<00:00,  9.21batch/s, loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 0.7517, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|██████████| 675/675 [01:22<00:00,  8.14batch/s, loss=0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 0.9178, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████| 675/675 [01:40<00:00,  6.72batch/s, loss=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 0.9421, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████| 675/675 [01:02<00:00, 10.87batch/s, loss=0.878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 0.9820, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████| 675/675 [01:02<00:00, 10.88batch/s, loss=0.894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 0.8787, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████| 675/675 [01:02<00:00, 10.86batch/s, loss=0.851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 0.8562, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████| 675/675 [01:02<00:00, 10.85batch/s, loss=0.876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 0.8296, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████| 675/675 [01:02<00:00, 10.88batch/s, loss=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 0.8332, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|██████████| 675/675 [01:02<00:00, 10.88batch/s, loss=0.836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 0.8270, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|██████████| 675/675 [01:02<00:00, 10.88batch/s, loss=0.844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 0.8025, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|██████████| 675/675 [01:02<00:00, 10.85batch/s, loss=0.809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 0.7910, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|██████████| 675/675 [01:18<00:00,  8.58batch/s, loss=0.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 0.7840, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|██████████| 675/675 [01:02<00:00, 10.87batch/s, loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 0.7616, Val Loss 0.7798, Dice 0.2202, IoU 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40:  55%|█████▌    | 373/675 [00:34<00:28, 10.72batch/s, loss=0.785]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_15318/764375841.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m             \u001B[0mepoch_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     72\u001B[0m             \u001B[0mpbar\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_postfix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'loss'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m             \u001B[0mpbar\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1f. Save and Visualize Training Metrics",
   "id": "59960d76c39ccc9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_metrics(train_losses, val_losses, val_dices, val_ious, dir_figures, name='metrics.png'):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    # Plot the training and validation losses\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the validation Dice Coefficient\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.plot(val_dices)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Dice Coefficient')\n",
    "    plt.title('Validation Dice Coefficient')\n",
    "\n",
    "    # Plot the validation IoU\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.plot(val_ious)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.title('Validation IoU')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(dir_figures, name))\n",
    "    plt.show()"
   ],
   "id": "e9ae3cd594f2f11c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Save metrics\n",
    "dir_figures = './tibo1/Lab3/figures'\n",
    "os.makedirs(dir_figures, exist_ok=True)\n",
    "save_metrics(train_losses, val_losses, val_dices, val_ious, dir_figures, name='metrics.png')"
   ],
   "id": "7dac774401c3b850"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Fine-Tuning with Amateur Data\n",
    "#### 2a. Load the Amateur Data"
   ],
   "id": "3a2841b5e885ceda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "amateur_X_train = np.load('./tibo1/Lab3/out/preprocessed/amateur_X_train.npy')\n",
    "amateur_y_train = np.load('./tibo1/Lab3/out/preprocessed/amateur_y_train.npy')\n",
    "amateur_X_val = np.load('./tibo1/Lab3/out/preprocessed/amateur_X_val.npy')\n",
    "amateur_y_val = np.load('./tibo1/Lab3/out/preprocessed/amateur_y_val.npy')\n"
   ],
   "id": "8692a5d4f983e17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2b. Load the Pre-Trained Model",
   "id": "fb88bbbb35dc521"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# best_model_path = './out/checkpoints/best_model.pth'  # Adjust as needed\n",
    "# model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "# print(\"Loaded pretrained model weights for fine-tuning.\")\n",
    "\n",
    "# Load weights to CPU first\n",
    "state_dict = torch.load(best_model_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "\n",
    "# Load the state dict into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)"
   ],
   "id": "3291015be9527e38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2c. Update the Dataloaders",
   "id": "707d98715afffd60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the number of CPU cores available\n",
    "#num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "train_dataset = MyDataset(amateur_X_train, amateur_y_train, transform=True)\n",
    "val_dataset = MyDataset(amateur_X_val, amateur_y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=5, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=5, pin_memory=True)\n",
    "\n",
    "print(f\"Using {num_workers} workers for DataLoader\")"
   ],
   "id": "e889bdd7ffbadf18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2d.Adjust Learning Rate for Fine-Tuning",
   "id": "5bf4b58278f51d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LEARNING_RATE = 1e-4  # Smaller learning rate for fine-tuning\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = LEARNING_RATE"
   ],
   "id": "6714534f1e415406"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2e.Continue Training Loop for Fine-Tuning",
   "id": "711e40b1ab7fa9aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Function to get the device\n",
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize directories and tracking variables\n",
    "checkpoint_dir = './tibo1/Lab3/out/checkpoints/'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "last_checkpoint = None\n",
    "last_epoch = 0\n",
    "\n",
    "# Search for the last checkpoint in the directory\n",
    "for file_name in os.listdir(checkpoint_dir):\n",
    "    match = re.match(r'fine_tune_checkpoint_epoch(\\d+)\\.pth', file_name)\n",
    "    if match:\n",
    "        epoch_num = int(match.group(1))\n",
    "        if epoch_num > last_epoch:\n",
    "            last_epoch = epoch_num\n",
    "            last_checkpoint = os.path.join(checkpoint_dir, file_name)\n",
    "\n",
    "# Resume training if a checkpoint exists\n",
    "if last_checkpoint:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    model.load_state_dict(torch.load(last_checkpoint))\n",
    "    optimizer.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'fine_tune_optimizer_epoch{last_epoch}.pth')))\n",
    "    print(f\"Resumed from epoch {last_epoch}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "# Update starting epoch\n",
    "start_epoch = last_epoch + 1\n",
    "EPOCHS_FINE_TUNE = 20  # Total number of epochs\n",
    "\n",
    "best_val_loss = float('inf')  # Reset best validation loss\n",
    "train_losses = globals().get('train_losses', [])  # Resume metrics if they exist\n",
    "val_losses = globals().get('val_losses', [])\n",
    "val_dices = globals().get('val_dices', [])\n",
    "val_ious = globals().get('val_ious', [])\n",
    "\n",
    "# Get device and setup GradScaler for mixed precision\n",
    "device = get_device()\n",
    "use_amp = device.type == 'cuda'\n",
    "scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "print(f\"Running fine-tuning on device: {device}\")\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(start_epoch, EPOCHS_FINE_TUNE + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(train_loader), desc=f'Fine-Tune Epoch {epoch}/{EPOCHS_FINE_TUNE}', unit='batch') as pbar:\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed precision training\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            pbar.update()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    dice_score = 0\n",
    "    iou_score = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            # Mixed precision validation\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute metrics\n",
    "            outputs = (outputs > 0.5).float()\n",
    "            dice_score += dice_coeff(outputs, masks).item()\n",
    "            iou_score += iou_coeff(outputs, masks).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    dice_score /= len(val_loader)\n",
    "    iou_score /= len(val_loader)\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(dice_score)\n",
    "\n",
    "    # Save metrics\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    val_losses.append(val_loss)\n",
    "    val_dices.append(dice_score)\n",
    "    val_ious.append(iou_score)\n",
    "\n",
    "    print(f'Fine-Tune Epoch {epoch}: Train Loss {train_losses[-1]:.4f}, Val Loss {val_losses[-1]:.4f}, '\n",
    "          f'Dice {dice_score:.4f}, IoU {iou_score:.4f}')\n",
    "\n",
    "    # Save model and optimizer checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'fine_tune_checkpoint_epoch{epoch}.pth'))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, f'fine_tune_optimizer_epoch{epoch}.pth'))\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = os.path.join(checkpoint_dir, f'best_fine_tuned_model_epoch{epoch}.pth')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Best model saved at epoch {epoch} with validation loss {best_val_loss:.4f}')\n"
   ],
   "id": "c5ec81d323174db7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save metrics\n",
    "dir_figures = './tibo1/Lab3/figures'\n",
    "os.makedirs(dir_figures, exist_ok=True)\n",
    "save_metrics(train_losses, val_losses, val_dices, val_ious, dir_figures, name='metrics_finetuned.png')"
   ],
   "id": "312c17496ada33db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "279fe0a8598372e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
