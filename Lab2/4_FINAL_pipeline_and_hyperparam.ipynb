{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:46:35.581644Z",
     "start_time": "2024-12-01T18:46:35.188837Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from biosppy.signals import ecg\n",
    "import scipy.signal as ss\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fftpack import fft\n",
    "from neurokit2 import ecg_invert\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:46:36.464758Z",
     "start_time": "2024-12-01T18:46:36.460482Z"
    }
   },
   "source": [
    "# Load data\n",
    "def load_data(train_path, test_path):\n",
    "    train = pd.read_csv(train_path, index_col=\"id\")\n",
    "    test = pd.read_csv(test_path, index_col=\"id\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def process_signal(ecg, signal_cols_len, wavelet='bior4.4', level=8):\n",
    "    ecg_signal = ecg\n",
    "    coeffs = pywt.wavedec(ecg_signal, wavelet, level=level)\n",
    "    coeffs[0] = np.zeros_like(coeffs[0])\n",
    "    coeffs[1] = np.zeros_like(coeffs[1])\n",
    "    coeffs[2] = np.zeros_like(coeffs[2])\n",
    "    denoised_signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "    corrected_valid_signal, _ = ecg_invert(denoised_signal[:len(ecg_signal)], sampling_rate=300, show=False)\n",
    "\n",
    "    std = corrected_valid_signal.std()\n",
    "    ret = (corrected_valid_signal - corrected_valid_signal.mean()) / std if std > 0 else corrected_valid_signal\n",
    "    return np.pad(ret, (0, signal_cols_len - len(ret)), 'constant', constant_values=(np.nan))\n",
    "\n",
    "\n",
    "def preprocess_signals(data):\n",
    "    signal_cols = [col for col in data.columns if col.startswith('x')]\n",
    "    l = len(signal_cols)\n",
    "    # Apply processing to each row (signal)\n",
    "    data[signal_cols] = data[signal_cols].apply(\n",
    "        lambda row: pd.Series(process_signal(row.dropna().values, l)), axis=1\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:47:01.918734Z",
     "start_time": "2024-12-01T18:46:41.263810Z"
    }
   },
   "source": [
    "# Load train and test data\n",
    "train_path = \"data/train.csv\"\n",
    "test_path = \"data/test.csv\"\n",
    "train, test = load_data(train_path, test_path)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:48:13.012986Z",
     "start_time": "2024-12-01T18:47:05.560754Z"
    }
   },
   "source": [
    "# Preprocess signals  (already done if inversion file)\n",
    "processed_train = preprocess_signals(train)\n",
    "processed_test = preprocess_signals(test)\n",
    "\n",
    "# Verify\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (5117, 17808), Test shape: (3411, 17807)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:51:39.413681Z",
     "start_time": "2024-12-01T18:51:39.403258Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_signal(signal, sampling_rate=300, title=\"ECG Signal\"):\n",
    "    \"\"\"\n",
    "    Plot an ECG signal.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: 1D array-like, the ECG signal to plot.\n",
    "    - sampling_rate: Integer, sampling rate of the signal (default=300 Hz).\n",
    "    - title: String, title of the plot.\n",
    "    \"\"\"\n",
    "    # Create a time axis based on the sampling rate\n",
    "    time = np.linspace(0, len(signal) / sampling_rate, len(signal))\n",
    "\n",
    "    # Plot the signal\n",
    "    plt.figure(figsize=(30, 6))\n",
    "    plt.plot(time, signal, label=\"ECG Signal\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats(signal):\n",
    "    signal = signal[~np.isnan(signal)]\n",
    "    if len(signal) == 0:\n",
    "        return [0, 0, 0, 0, 0]\n",
    "    if len(signal) == 1:\n",
    "        return [signal[0], 0, signal[0], signal[0], signal[0]]\n",
    "    return [np.mean(signal), np.std(signal), np.median(signal), min(signal), max(signal)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(signal, sampling_rate=300):\n",
    "    #try:\n",
    "    features = []\n",
    "    # ECG Processing\n",
    "    _, filtered, rpeaks, _, _, _, heart_rate = ecg(signal, sampling_rate=sampling_rate, show=False)\n",
    "    filtered = filtered[~np.isnan(filtered)]\n",
    "    rpeaks = rpeaks[~np.isnan(rpeaks)]\n",
    "    # RR Intervals\n",
    "    rr_intervals = np.diff(rpeaks) / sampling_rate\n",
    "\n",
    "    features.append(extract_stats(rr_intervals))\n",
    "\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(rr_intervals))))\n",
    "    sdnn = np.std(rr_intervals)\n",
    "    cvsd = rmssd / sdnn if sdnn > 0 else 0\n",
    "    pnn50 = np.sum(np.abs(np.diff(rr_intervals)) > 0.05) / len(rr_intervals)\n",
    "\n",
    "    features.append([rmssd, sdnn, cvsd, pnn50])\n",
    "\n",
    "    delineation = ecg_delineate(filtered, rpeaks, sampling_rate)[1]\n",
    "    q_peaks = delineation.get(\"ECG_Q_Peaks\", [])\n",
    "    s_peaks = delineation.get(\"ECG_S_Peaks\", [])\n",
    "    p_peaks = delineation.get(\"ECG_P_Peaks\", [])\n",
    "    t_peaks = delineation.get(\"ECG_T_Peaks\", [])\n",
    "\n",
    "    # Clean peaks\n",
    "    valid_q = ~np.isnan(q_peaks)\n",
    "    valid_s = ~np.isnan(s_peaks)\n",
    "    valid_p = ~np.isnan(p_peaks)\n",
    "    valid_t = ~np.isnan(t_peaks)\n",
    "    q_peaks, s_peaks = np.array(q_peaks)[valid_q].astype(int), np.array(s_peaks)[valid_s].astype(int)\n",
    "    p_peaks, t_peaks = np.array(p_peaks)[valid_p].astype(int), np.array(t_peaks)[valid_t].astype(int)\n",
    "\n",
    "    pr_intervals_size = min(len(rpeaks), len(p_peaks))\n",
    "    qt_intervals_size = min(len(q_peaks), len(t_peaks))\n",
    "    ps_intervals_size = min(len(p_peaks), len(s_peaks))\n",
    "\n",
    "    pr_intervals = rpeaks[:pr_intervals_size] - p_peaks[:pr_intervals_size]\n",
    "    qt_intervals = t_peaks[:qt_intervals_size] - q_peaks[:qt_intervals_size]\n",
    "    ps_intervals = s_peaks[:ps_intervals_size] - p_peaks[:ps_intervals_size]\n",
    "\n",
    "    features.append(extract_stats(pr_intervals))\n",
    "    features.append(extract_stats(qt_intervals))\n",
    "    features.append(extract_stats(ps_intervals))\n",
    "\n",
    "    # Amplitude Features\n",
    "\n",
    "    features.append(extract_stats(filtered[rpeaks]))\n",
    "    features.append(extract_stats(filtered[p_peaks]))\n",
    "    features.append(extract_stats(filtered[s_peaks]))\n",
    "    features.append(extract_stats(filtered[t_peaks]))\n",
    "    features.append(extract_stats(filtered[q_peaks]))\n",
    "\n",
    "    # Non Linear\n",
    "\n",
    "    features.append(pyent.sample_entropy(signal, 2, 0.2 * np.nanstd(signal))[0] if len(signal) > 0 else 0)\n",
    "    features.append(np.nanstd(rr_intervals) / np.sqrt(2) if len(rr_intervals) > 0 else 0)\n",
    "    features.append(np.sqrt(2) * np.nanstd(rr_intervals) if len(rr_intervals) > 0 else 0)\n",
    "\n",
    "    artifact_ratio = np.sum(np.abs(signal) > 3 * np.nanstd(signal)) / max(len(signal), 1)\n",
    "    features.append(artifact_ratio)\n",
    "\n",
    "    # Frequency-Domain Features\n",
    "    freq = np.fft.rfftfreq(len(signal), 1 / sampling_rate)\n",
    "    spectrum = np.abs(np.fft.rfft(signal))\n",
    "    low_freq_band = (freq >= 0.04) & (freq <= 0.15)\n",
    "    high_freq_band = (freq >= 0.15) & (freq <= 0.4)\n",
    "\n",
    "    features.append(\n",
    "        [freq[np.argmax(spectrum)], np.nanmean(spectrum), np.nanstd(spectrum), np.sum(spectrum[low_freq_band]),\n",
    "         np.sum(spectrum[high_freq_band]),\n",
    "         np.sum(spectrum[low_freq_band]) / (np.sum(spectrum[high_freq_band]) + 1e-10)])\n",
    "    # Autocorrelation Features\n",
    "    autocorr = np.correlate(signal, signal, mode=\"full\") / len(signal)\n",
    "    autocorr = autocorr[autocorr.size // 2:]\n",
    "    features.append(extract_stats(autocorr))\n",
    "\n",
    "    features.append([np.ptp(signal), kurtosis(signal), skew(signal)])\n",
    "\n",
    "    # Wavelet Transform Features\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=5)\n",
    "    for i, coeff in enumerate(coeffs[1:], start=1):\n",
    "        features.append(np.sum(np.square(coeff)))\n",
    "\n",
    "    features_flat = []\n",
    "    for f in features:\n",
    "        if isinstance(f, list):\n",
    "            features_flat.extend(f)\n",
    "        else:\n",
    "            features_flat.append(f)\n",
    "\n",
    "    f = {index: value for index, value in enumerate(features_flat)}\n",
    "    return f\n",
    "\n",
    "#except Exception as e:\n",
    "#    print(e)\n",
    "#    return {index: value for index, value in enumerate([0] * 63)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:42:31.140443Z",
     "start_time": "2024-11-25T16:33:22.888562Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_row(row, signal_cols, sampling_rate):\n",
    "    \"\"\"\n",
    "    Process a single row to extract features.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): Single row of the DataFrame.\n",
    "        signal_cols (list): List of signal column names.\n",
    "        sampling_rate (int): Sampling rate for feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted features for the row.\n",
    "    \"\"\"\n",
    "    signal = row[signal_cols].dropna().to_numpy(dtype=\"float32\")\n",
    "    features = extract_features(signal, sampling_rate=sampling_rate)  # Use your existing `extract_features` function\n",
    "    features[\"id\"] = row.name\n",
    "    if \"y\" in row:\n",
    "        features[\"label\"] = row[\"y\"]\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features_from_df_parallel(data, sampling_rate=300, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Extract features from a DataFrame in parallel.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing signal data.\n",
    "        sampling_rate (int): Sampling rate for feature extraction.\n",
    "        n_jobs (int): Number of CPU cores to use (-1 for all cores).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of extracted features.\n",
    "    \"\"\"\n",
    "    signal_cols = [col for col in data.columns if col.startswith('x')]\n",
    "\n",
    "    # Use Parallel for efficient processing\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_row)(row, signal_cols, sampling_rate)\n",
    "        for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Extracting Features in Parallel\")\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features in Parallel: 100%|██████████| 5117/5117 [18:14<00:00,  4.67it/s]\n",
      "Extracting Features in Parallel: 100%|██████████| 3411/3411 [12:40<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features for train and test\n",
    "train_features = extract_features_from_df_parallel(processed_train, sampling_rate=300)\n",
    "test_features = extract_features_from_df_parallel(processed_test, sampling_rate=300)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training (Random Params) - 80% Train, 20% Validation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:32:15.983956Z",
     "start_time": "2024-12-01T22:31:53.919789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import VotingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Prepare data\n",
    "train_features = pd.read_csv(\"features/final_train_features.csv\")\n",
    "test_features = pd.read_csv(\"features/final_test_features.csv\")\n",
    "\n",
    "X = train_features.fillna(0).drop(columns=[\"id\", \"label\"])  # Features\n",
    "y = train_features[\"label\"]  # Target\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define individual classifiers\n",
    "svm_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='rbf', C=3.9495, probability=True, random_state=42, class_weight='balanced')\n",
    ")\n",
    "gb_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    HistGradientBoostingClassifier(\n",
    "        learning_rate=0.013,\n",
    "        max_iter=212,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        l2_regularization=0.2\n",
    "    )\n",
    ")\n",
    "xgb_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    XGBClassifier(\n",
    "        learning_rate=0.2,\n",
    "        max_depth=7,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_pipeline),\n",
    "        ('gb', gb_pipeline),\n",
    "        ('xgb', xgb_pipeline)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Define pipeline with SMOTE and ensemble\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', ensemble)\n",
    "])\n",
    "\n",
    "# Perform Stratified K-Fold Cross-Validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Performing cross-validation...\")\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_kfold, scoring=\"f1_micro\", n_jobs=-1)\n",
    "print(f\"Cross-Validation F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1 Score: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# Train and evaluate on validation set\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "f1 = f1_score(y_val, y_pred, average=\"micro\")\n",
    "print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cross-validation...\n",
      "Cross-Validation F1 Scores: [0.83028083 0.82905983 0.82051282 0.80562347 0.83251834]\n",
      "Mean CV F1 Score: 0.8236 ± 0.0099\n",
      "Validation F1 Score: 0.8242\n",
      "Confusion Matrix:\n",
      "[[557   1  43   5]\n",
      " [  2  64  19   4]\n",
      " [ 70  19 197   9]\n",
      " [  5   2   1  26]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.92      0.90       606\n",
      "         1.0       0.74      0.72      0.73        89\n",
      "         2.0       0.76      0.67      0.71       295\n",
      "         3.0       0.59      0.76      0.67        34\n",
      "\n",
      "    accuracy                           0.82      1024\n",
      "   macro avg       0.74      0.77      0.75      1024\n",
      "weighted avg       0.82      0.82      0.82      1024\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "Performing cross-validation...\n",
    "Cross-Validation F1 Scores: [0.83028083 0.82905983 0.82051282 0.80562347 0.83251834]\n",
    "Mean CV F1 Score: 0.8236 ± 0.0099\n",
    "Validation F1 Score: 0.8242\n",
    "Confusion Matrix:\n",
    "[[557   1  43   5]\n",
    " [  2  64  19   4]\n",
    " [ 70  19 197   9]\n",
    " [  5   2   1  26]]\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.88      0.92      0.90       606\n",
    "         1.0       0.74      0.72      0.73        89\n",
    "         2.0       0.76      0.67      0.71       295\n",
    "         3.0       0.59      0.76      0.67        34\n",
    "\n",
    "    accuracy                           0.82      1024\n",
    "   macro avg       0.74      0.77      0.75      1024\n",
    "weighted avg       0.82      0.82      0.82      1024\n",
    "\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training (Grid Search) - 80% Train, 20% Validation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T00:05:43.467214Z",
     "start_time": "2024-12-02T00:05:09.413551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'classifier__svm__svc__C': [8.5],\n",
    "    'classifier__gb__histgradientboostingclassifier__learning_rate': [0.1],\n",
    "    'classifier__gb__histgradientboostingclassifier__max_iter': [100],\n",
    "    'classifier__xgb__xgbclassifier__max_depth': [10],\n",
    "    'classifier__xgb__xgbclassifier__learning_rate': [0.2],\n",
    "\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_micro\",\n",
    "    cv=stratified_kfold,\n",
    "    verbose=100,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_val)\n",
    "val_score = f1_score(y_val, y_pred, average=\"micro\")\n",
    "print(f\"Validation F1 Score: {val_score:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Parameters: {'classifier__gb__histgradientboostingclassifier__learning_rate': 0.1, 'classifier__gb__histgradientboostingclassifier__max_iter': 100, 'classifier__svm__svc__C': 8.5, 'classifier__xgb__xgbclassifier__learning_rate': 0.2, 'classifier__xgb__xgbclassifier__max_depth': 10}\n",
      "Best CV Score: 0.8309\n",
      "Validation F1 Score: 0.8281\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "Starting Grid Search...\n",
    "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
    "Best Parameters: {'classifier__gb__histgradientboostingclassifier__learning_rate': 0.1, 'classifier__gb__histgradientboostingclassifier__max_iter': 100, 'classifier__svm__svc__C': 8.5, 'classifier__xgb__xgbclassifier__learning_rate': 0.2, 'classifier__xgb__xgbclassifier__max_depth': 10}\n",
    "Best CV Score: 0.8309\n",
    "Validation F1 Score: 0.8281\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Evaluation - 100% Train, Test"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T00:08:12.358656Z",
     "start_time": "2024-12-02T00:07:46.869240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply SMOTE to the entire dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)  # 100% dataset\n",
    "\n",
    "best_param = grid_search.best_params_\n",
    "print(best_param)\n",
    "# Train final model with best parameters\n",
    "pipeline.set_params(**best_param)\n",
    "\n",
    "# Perform cross-validation on the full dataset\n",
    "cv_scores_final = cross_val_score(pipeline, X, y, cv=stratified_kfold, scoring=\"f1_micro\", n_jobs=-1)\n",
    "print(f\"Final CV F1 Scores: {cv_scores_final.mean():.4f} ± {cv_scores_final.std():.4f}\")\n",
    "\n",
    "# Train the model on the entire resampled (SMOTE) dataset\n",
    "pipeline.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "X_test = test_features.fillna(0).drop(columns=[\"id\"])\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_features[\"id\"],\n",
    "    \"y\": y_test_pred\n",
    "})\n",
    "submission.to_csv(\"./out/submission_final2_SVM_GB_XGB_Voting_SMOTE_GRIDSEARCH.csv\", index=False)\n",
    "print(\"Submission file created!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__gb__histgradientboostingclassifier__learning_rate': 0.1, 'classifier__gb__histgradientboostingclassifier__max_iter': 100, 'classifier__svm__svc__C': 8.5, 'classifier__xgb__xgbclassifier__learning_rate': 0.2, 'classifier__xgb__xgbclassifier__max_depth': 10}\n",
      "Final CV F1 Scores: 0.8310 ± 0.0064\n",
      "Submission file created!\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
